{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pipeline Research\n",
    "when developing a pipeline, I use this notebook to play around with the data and see what I can do with it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pipeline.keyword_extractor import DefaultKeywordExtractor\n",
    "from datasets.docs_dataset import DocsDataset\n",
    "from datasets.stock_dataset import StockMeta\n",
    "from pipeline.docs_filterer import DefaultFilterer\n",
    "from pipeline.docs_labeler import DefaultDocsLabeler\n",
    "\n",
    "doc_dataset = DocsDataset(documents_csv_path=\"./organized_data/documents.csv\")\n",
    "stock_meta = StockMeta(stock_meta_path=\"./organized_data/stock_metadata.csv\")\n",
    "\n",
    "filterer = DefaultFilterer()\n",
    "labeler = DefaultDocsLabeler(s=3)\n",
    "keyword_extractor = DefaultKeywordExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "stock_name = '台積電'\n",
    "stock = stock_meta.get_stock_by_name(stock_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "filtered_docs = filterer.filter_documents(doc_dataset, stock)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "labeled_docs = labeler.label_documents(filtered_docs, stock)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\break\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.510 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m labeled_keyword_docs \u001B[38;5;241m=\u001B[39m \u001B[43mkeyword_extractor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_keywords\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabeled_docs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Projects\\College\\大四下\\DatAnalysis\\2023-2-data-analyze-midterm\\pipeline\\keyword_extractor.py:37\u001B[0m, in \u001B[0;36mDefaultKeywordExtractor.extract_keywords\u001B[1;34m(self, labeled_docs)\u001B[0m\n\u001B[0;32m     35\u001B[0m     doc_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m;\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([doc\u001B[38;5;241m.\u001B[39mtitle, doc\u001B[38;5;241m.\u001B[39mauthor, doc\u001B[38;5;241m.\u001B[39mcontent])\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;66;03m# extract the keywords\u001B[39;00m\n\u001B[1;32m---> 37\u001B[0m     keywords \u001B[38;5;241m=\u001B[39m \u001B[43mjieba\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43manalyse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtextrank\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc_str\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtopK\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtopK\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwithWeight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     38\u001B[0m     doc_keywords\u001B[38;5;241m.\u001B[39mappend(keywords)\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m LabelDataset(doc_keywords, labeled_docs\u001B[38;5;241m.\u001B[39mlabels)\n",
      "File \u001B[1;32m~\\Projects\\College\\大四下\\DatAnalysis\\2023-2-data-analyze-midterm\\venv\\Lib\\site-packages\\jieba\\analyse\\textrank.py:84\u001B[0m, in \u001B[0;36mTextRank.textrank\u001B[1;34m(self, sentence, topK, withWeight, allowPOS, withFlag)\u001B[0m\n\u001B[0;32m     82\u001B[0m g \u001B[38;5;241m=\u001B[39m UndirectWeightedGraph()\n\u001B[0;32m     83\u001B[0m cm \u001B[38;5;241m=\u001B[39m defaultdict(\u001B[38;5;28mint\u001B[39m)\n\u001B[1;32m---> 84\u001B[0m words \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mcut(sentence))\n\u001B[0;32m     85\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, wp \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(words):\n\u001B[0;32m     86\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpairfilter(wp):\n",
      "File \u001B[1;32m~\\Projects\\College\\大四下\\DatAnalysis\\2023-2-data-analyze-midterm\\venv\\Lib\\site-packages\\jieba\\posseg\\__init__.py:249\u001B[0m, in \u001B[0;36mPOSTokenizer.cut\u001B[1;34m(self, sentence, HMM)\u001B[0m\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcut\u001B[39m(\u001B[38;5;28mself\u001B[39m, sentence, HMM\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m--> 249\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__cut_internal(sentence, HMM\u001B[38;5;241m=\u001B[39mHMM):\n\u001B[0;32m    250\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m w\n",
      "File \u001B[1;32m~\\Projects\\College\\大四下\\DatAnalysis\\2023-2-data-analyze-midterm\\venv\\Lib\\site-packages\\jieba\\posseg\\__init__.py:226\u001B[0m, in \u001B[0;36mPOSTokenizer.__cut_internal\u001B[1;34m(self, sentence, HMM)\u001B[0m\n\u001B[0;32m    224\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m blk \u001B[38;5;129;01min\u001B[39;00m blocks:\n\u001B[0;32m    225\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m re_han_internal\u001B[38;5;241m.\u001B[39mmatch(blk):\n\u001B[1;32m--> 226\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m cut_blk(blk):\n\u001B[0;32m    227\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m word\n\u001B[0;32m    228\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Projects\\College\\大四下\\DatAnalysis\\2023-2-data-analyze-midterm\\venv\\Lib\\site-packages\\jieba\\posseg\\__init__.py:209\u001B[0m, in \u001B[0;36mPOSTokenizer.__cut_DAG\u001B[1;34m(self, sentence)\u001B[0m\n\u001B[0;32m    207\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mFREQ\u001B[38;5;241m.\u001B[39mget(buf):\n\u001B[0;32m    208\u001B[0m     recognized \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__cut_detail(buf)\n\u001B[1;32m--> 209\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m recognized:\n\u001B[0;32m    210\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m t\n\u001B[0;32m    211\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Projects\\College\\大四下\\DatAnalysis\\2023-2-data-analyze-midterm\\venv\\Lib\\site-packages\\jieba\\posseg\\__init__.py:139\u001B[0m, in \u001B[0;36mPOSTokenizer.__cut_detail\u001B[1;34m(self, sentence)\u001B[0m\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m blk \u001B[38;5;129;01min\u001B[39;00m blocks:\n\u001B[0;32m    138\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m re_han_detail\u001B[38;5;241m.\u001B[39mmatch(blk):\n\u001B[1;32m--> 139\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__cut(blk):\n\u001B[0;32m    140\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m word\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\Projects\\College\\大四下\\DatAnalysis\\2023-2-data-analyze-midterm\\venv\\Lib\\site-packages\\jieba\\posseg\\__init__.py:118\u001B[0m, in \u001B[0;36mPOSTokenizer.__cut\u001B[1;34m(self, sentence)\u001B[0m\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__cut\u001B[39m(\u001B[38;5;28mself\u001B[39m, sentence):\n\u001B[1;32m--> 118\u001B[0m     prob, pos_list \u001B[38;5;241m=\u001B[39m \u001B[43mviterbi\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    119\u001B[0m \u001B[43m        \u001B[49m\u001B[43msentence\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchar_state_tab_P\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_P\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrans_P\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43memit_P\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    120\u001B[0m     begin, nexti \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m    122\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, char \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(sentence):\n",
      "File \u001B[1;32m~\\Projects\\College\\大四下\\DatAnalysis\\2023-2-data-analyze-midterm\\venv\\Lib\\site-packages\\jieba\\posseg\\viterbi.py:28\u001B[0m, in \u001B[0;36mviterbi\u001B[1;34m(obs, states, start_p, trans_p, emit_p)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m#prev_states = get_top_states(V[t-1])\u001B[39;00m\n\u001B[0;32m     25\u001B[0m prev_states \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     26\u001B[0m     x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m mem_path[t \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mkeys() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(trans_p[x]) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m---> 28\u001B[0m prev_states_expect_next \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(\n\u001B[0;32m     29\u001B[0m     (y \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m prev_states \u001B[38;5;28;01mfor\u001B[39;00m y \u001B[38;5;129;01min\u001B[39;00m trans_p[x]\u001B[38;5;241m.\u001B[39mkeys()))\n\u001B[0;32m     30\u001B[0m obs_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(\n\u001B[0;32m     31\u001B[0m     states\u001B[38;5;241m.\u001B[39mget(obs[t], all_states)) \u001B[38;5;241m&\u001B[39m prev_states_expect_next\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m obs_states:\n",
      "File \u001B[1;32m~\\Projects\\College\\大四下\\DatAnalysis\\2023-2-data-analyze-midterm\\venv\\Lib\\site-packages\\jieba\\posseg\\viterbi.py:29\u001B[0m, in \u001B[0;36m<genexpr>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m#prev_states = get_top_states(V[t-1])\u001B[39;00m\n\u001B[0;32m     25\u001B[0m prev_states \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     26\u001B[0m     x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m mem_path[t \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mkeys() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(trans_p[x]) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     28\u001B[0m prev_states_expect_next \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(\n\u001B[1;32m---> 29\u001B[0m     (y \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m prev_states \u001B[38;5;28;01mfor\u001B[39;00m y \u001B[38;5;129;01min\u001B[39;00m trans_p[x]\u001B[38;5;241m.\u001B[39mkeys()))\n\u001B[0;32m     30\u001B[0m obs_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(\n\u001B[0;32m     31\u001B[0m     states\u001B[38;5;241m.\u001B[39mget(obs[t], all_states)) \u001B[38;5;241m&\u001B[39m prev_states_expect_next\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m obs_states:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "labeled_keyword_docs = keyword_extractor.extract_keywords(labeled_docs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "doc_dataset[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import jieba.analyse\n",
    "\n",
    "doc = doc_dataset[100]\n",
    "doc_str = \";\".join([doc.title, doc.author, doc.content])\n",
    "# extract the top 10 keywords\n",
    "keywords = jieba.analyse.extract_tags(doc_str, topK=30, withWeight=True)\n",
    "\n",
    "# print the keywords and their weights\n",
    "print('Keywords:')\n",
    "for keyword, weight in keywords:\n",
    "    print(keyword, weight)\n",
    "\n",
    "# extract the top 5 topics\n",
    "topics = jieba.analyse.textrank(doc_str, topK=20, withWeight=True)\n",
    "\n",
    "# print the topics and their weights\n",
    "print('Topics:')\n",
    "for topic, weight in topics:\n",
    "    print(topic, weight)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
