{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Pipeline Research\n",
    "when developing a pipeline, I use this notebook to play around with the data and see what I can do with it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pipeline.keyword_extractor import DefaultKeywordExtractor\n",
    "from datasets.docs_dataset import DocsDataset, DbDocsDataset\n",
    "from datasets.stock_dataset import StockMeta\n",
    "from pipeline.docs_filterer import DefaultFilterer\n",
    "from pipeline.docs_labeler import DefaultDocsLabeler\n",
    "from pipeline.vectorlizer import TFIDFVectorlizer\n",
    "from pipeline.labeled_docs_filterer import Near0returnFilterer\n",
    "from utils.data import random_split_train_val\n",
    "\n",
    "# doc_dataset = DocsDataset(documents_csv_path=\"./organized_data/documents.csv\")\n",
    "doc_dataset = DbDocsDataset()\n",
    "stock_meta = StockMeta(stock_meta_path=\"./organized_data/stock_metadata.csv\")\n",
    "\n",
    "filterer = DefaultFilterer()\n",
    "labeler = DefaultDocsLabeler(s=3)\n",
    "label_docs_filterer = Near0returnFilterer(threshold=5)\n",
    "keyword_extractor = DefaultKeywordExtractor()\n",
    "vectorlizer = TFIDFVectorlizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "stock_name = '台積電'\n",
    "stock = stock_meta.get_stock_by_name(stock_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "filtering documents: 100%|██████████| 1181305/1181305 [00:32<00:00, 36295.41it/s]\n"
     ]
    }
   ],
   "source": [
    "filtered_docs = filterer.filter_documents(doc_dataset, stock)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "labeling documents: 1026it [00:01, 781.58it/s]\n"
     ]
    }
   ],
   "source": [
    "labeled_docs = labeler.label_documents(filtered_docs, stock)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filtered_labeled_docs = label_docs_filterer.filter(labeled_docs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "extracting keywords:   0%|          | 0/60889 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/36/xrq0hvyj4t1g49sh7nhx56c00000gn/T/jieba.cache\n",
      "Loading model cost 0.319 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "extracting keywords:   1%|          | 612/60889 [03:42<6:05:15,  2.75it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m labeled_keyword_docs \u001B[38;5;241m=\u001B[39m \u001B[43mkeyword_extractor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract_keywords\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabeled_docs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Developer/Projects/College/大四下/數據分析/期中/pipeline/keyword_extractor.py:44\u001B[0m, in \u001B[0;36mDefaultKeywordExtractor.extract_keywords\u001B[0;34m(self, labeled_docs, verbose)\u001B[0m\n\u001B[1;32m     42\u001B[0m doc_str \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m;\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([doc\u001B[38;5;241m.\u001B[39mtitle, doc\u001B[38;5;241m.\u001B[39mauthor, doc\u001B[38;5;241m.\u001B[39mcontent])\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# extract the keywords\u001B[39;00m\n\u001B[0;32m---> 44\u001B[0m keywords \u001B[38;5;241m=\u001B[39m \u001B[43mjieba\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43manalyse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtextrank\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc_str\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtopK\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwithWeight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;66;03m# record the keywords\u001B[39;00m\n\u001B[1;32m     46\u001B[0m doc\u001B[38;5;241m.\u001B[39mkeywords \u001B[38;5;241m=\u001B[39m keywords\n",
      "File \u001B[0;32m~/Developer/Projects/College/大四下/數據分析/期中/venv/lib/python3.11/site-packages/jieba/analyse/textrank.py:84\u001B[0m, in \u001B[0;36mTextRank.textrank\u001B[0;34m(self, sentence, topK, withWeight, allowPOS, withFlag)\u001B[0m\n\u001B[1;32m     82\u001B[0m g \u001B[38;5;241m=\u001B[39m UndirectWeightedGraph()\n\u001B[1;32m     83\u001B[0m cm \u001B[38;5;241m=\u001B[39m defaultdict(\u001B[38;5;28mint\u001B[39m)\n\u001B[0;32m---> 84\u001B[0m words \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mcut(sentence))\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, wp \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(words):\n\u001B[1;32m     86\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpairfilter(wp):\n",
      "File \u001B[0;32m~/Developer/Projects/College/大四下/數據分析/期中/venv/lib/python3.11/site-packages/jieba/posseg/__init__.py:249\u001B[0m, in \u001B[0;36mPOSTokenizer.cut\u001B[0;34m(self, sentence, HMM)\u001B[0m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcut\u001B[39m(\u001B[38;5;28mself\u001B[39m, sentence, HMM\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m--> 249\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__cut_internal(sentence, HMM\u001B[38;5;241m=\u001B[39mHMM):\n\u001B[1;32m    250\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m w\n",
      "File \u001B[0;32m~/Developer/Projects/College/大四下/數據分析/期中/venv/lib/python3.11/site-packages/jieba/posseg/__init__.py:226\u001B[0m, in \u001B[0;36mPOSTokenizer.__cut_internal\u001B[0;34m(self, sentence, HMM)\u001B[0m\n\u001B[1;32m    224\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m blk \u001B[38;5;129;01min\u001B[39;00m blocks:\n\u001B[1;32m    225\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m re_han_internal\u001B[38;5;241m.\u001B[39mmatch(blk):\n\u001B[0;32m--> 226\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m cut_blk(blk):\n\u001B[1;32m    227\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m word\n\u001B[1;32m    228\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Developer/Projects/College/大四下/數據分析/期中/venv/lib/python3.11/site-packages/jieba/posseg/__init__.py:209\u001B[0m, in \u001B[0;36mPOSTokenizer.__cut_DAG\u001B[0;34m(self, sentence)\u001B[0m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer\u001B[38;5;241m.\u001B[39mFREQ\u001B[38;5;241m.\u001B[39mget(buf):\n\u001B[1;32m    208\u001B[0m     recognized \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__cut_detail(buf)\n\u001B[0;32m--> 209\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m recognized:\n\u001B[1;32m    210\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m t\n\u001B[1;32m    211\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Developer/Projects/College/大四下/數據分析/期中/venv/lib/python3.11/site-packages/jieba/posseg/__init__.py:139\u001B[0m, in \u001B[0;36mPOSTokenizer.__cut_detail\u001B[0;34m(self, sentence)\u001B[0m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m blk \u001B[38;5;129;01min\u001B[39;00m blocks:\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m re_han_detail\u001B[38;5;241m.\u001B[39mmatch(blk):\n\u001B[0;32m--> 139\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__cut(blk):\n\u001B[1;32m    140\u001B[0m             \u001B[38;5;28;01myield\u001B[39;00m word\n\u001B[1;32m    141\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Developer/Projects/College/大四下/數據分析/期中/venv/lib/python3.11/site-packages/jieba/posseg/__init__.py:118\u001B[0m, in \u001B[0;36mPOSTokenizer.__cut\u001B[0;34m(self, sentence)\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__cut\u001B[39m(\u001B[38;5;28mself\u001B[39m, sentence):\n\u001B[0;32m--> 118\u001B[0m     prob, pos_list \u001B[38;5;241m=\u001B[39m \u001B[43mviterbi\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    119\u001B[0m \u001B[43m        \u001B[49m\u001B[43msentence\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchar_state_tab_P\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_P\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrans_P\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43memit_P\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    120\u001B[0m     begin, nexti \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    122\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, char \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(sentence):\n",
      "File \u001B[0;32m~/Developer/Projects/College/大四下/數據分析/期中/venv/lib/python3.11/site-packages/jieba/posseg/viterbi.py:37\u001B[0m, in \u001B[0;36mviterbi\u001B[0;34m(obs, states, start_p, trans_p, emit_p)\u001B[0m\n\u001B[1;32m     34\u001B[0m     obs_states \u001B[38;5;241m=\u001B[39m prev_states_expect_next \u001B[38;5;28;01mif\u001B[39;00m prev_states_expect_next \u001B[38;5;28;01melse\u001B[39;00m all_states\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m y \u001B[38;5;129;01min\u001B[39;00m obs_states:\n\u001B[0;32m---> 37\u001B[0m     prob, state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mmax\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mV\u001B[49m\u001B[43m[\u001B[49m\u001B[43mt\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43my0\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtrans_p\u001B[49m\u001B[43m[\u001B[49m\u001B[43my0\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mMIN_INF\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\n\u001B[1;32m     38\u001B[0m \u001B[43m                       \u001B[49m\u001B[43memit_p\u001B[49m\u001B[43m[\u001B[49m\u001B[43my\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m[\u001B[49m\u001B[43mt\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mMIN_FLOAT\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my0\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43my0\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mprev_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m     V[t][y] \u001B[38;5;241m=\u001B[39m prob\n\u001B[1;32m     40\u001B[0m     mem_path[t][y] \u001B[38;5;241m=\u001B[39m state\n",
      "File \u001B[0;32m~/Developer/Projects/College/大四下/數據分析/期中/venv/lib/python3.11/site-packages/jieba/posseg/viterbi.py:37\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     34\u001B[0m     obs_states \u001B[38;5;241m=\u001B[39m prev_states_expect_next \u001B[38;5;28;01mif\u001B[39;00m prev_states_expect_next \u001B[38;5;28;01melse\u001B[39;00m all_states\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m y \u001B[38;5;129;01min\u001B[39;00m obs_states:\n\u001B[0;32m---> 37\u001B[0m     prob, state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m((V[t \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m][y0] \u001B[38;5;241m+\u001B[39m trans_p[y0]\u001B[38;5;241m.\u001B[39mget(y, MIN_INF) \u001B[38;5;241m+\u001B[39m\n\u001B[1;32m     38\u001B[0m                        emit_p[y]\u001B[38;5;241m.\u001B[39mget(obs[t], MIN_FLOAT), y0) \u001B[38;5;28;01mfor\u001B[39;00m y0 \u001B[38;5;129;01min\u001B[39;00m prev_states)\n\u001B[1;32m     39\u001B[0m     V[t][y] \u001B[38;5;241m=\u001B[39m prob\n\u001B[1;32m     40\u001B[0m     mem_path[t][y] \u001B[38;5;241m=\u001B[39m state\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "labeled_keyword_docs = keyword_extractor.extract_keywords(filtered_labeled_docs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labeled_vectors = vectorlizer.convert(labeled_keyword_docs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "Shape of the TF-IDF matrix: (4, 9)\n",
      "TF-IDF matrix:\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset = random_split_train_val(labeled_vectors, 0.8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to convert continuous labels to binary\n",
    "def convert_to_binary_labels(y_values):\n",
    "    return [1 if y > 0 else 0 for y in y_values]\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split_train_val(labeled_vectors, 0.8)\n",
    "\n",
    "# Extract features and labels from the training and validation datasets\n",
    "X_train, y_train = zip(*train_dataset)\n",
    "X_val, y_val = zip(*val_dataset)\n",
    "\n",
    "# Train the SVR model\n",
    "model = SVR()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the training and validation datasets\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "# Convert predictions to binary labels for evaluation\n",
    "y_train_pred_binary = convert_to_binary_labels(y_train_pred)\n",
    "y_val_pred_binary = convert_to_binary_labels(y_val_pred)\n",
    "\n",
    "# Convert true labels to binary for evaluation\n",
    "y_train_binary = convert_to_binary_labels(y_train)\n",
    "y_val_binary = convert_to_binary_labels(y_val)\n",
    "\n",
    "# Calculate performance metrics\n",
    "train_acc = accuracy_score(y_train_binary, y_train_pred_binary)\n",
    "val_acc = accuracy_score(y_val_binary, y_val_pred_binary)\n",
    "\n",
    "train_report = classification_report(y_train_binary, y_train_pred_binary)\n",
    "val_report = classification_report(y_val_binary, y_val_pred_binary)\n",
    "\n",
    "train_conf_matrix = confusion_matrix(y_train_binary, y_train_pred_binary)\n",
    "val_conf_matrix = confusion_matrix(y_val_binary, y_val_pred_binary)\n",
    "\n",
    "# Display the results\n",
    "print(\"Training accuracy:\", train_acc)\n",
    "print(\"Validation accuracy:\", val_acc)\n",
    "\n",
    "print(\"\\nTraining classification report:\")\n",
    "print(train_report)\n",
    "\n",
    "print(\"\\nValidation classification report:\")\n",
    "print(val_report)\n",
    "\n",
    "print(\"\\nTraining confusion matrix:\")\n",
    "print(train_conf_matrix)\n",
    "\n",
    "print(\"\\nValidation confusion matrix:\")\n",
    "print(val_conf_matrix)\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ConfusionMatrixDisplay(train_conf_matrix, display_labels=np.unique(y_train_binary)).plot(ax=ax1, cmap='Blues', xticks_rotation=45)\n",
    "ax1.set_title(\"Training Confusion Matrix\")\n",
    "\n",
    "ConfusionMatrixDisplay(val_conf_matrix, display_labels=np.unique(y_val_binary)).plot(ax=ax2, cmap='Blues', xticks_rotation=45)\n",
    "ax2.set_title(\"Validation Confusion Matrix\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
